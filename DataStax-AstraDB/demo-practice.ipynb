{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['ASTRA_DB_API_ENDPOINT'] = os.getenv('ASTRA_DB_API_ENDPOINT')\n",
    "os.environ['ASTRA_DB_APPLICATION_TOKEN'] = os.getenv('ASTRA_DB_APPLICATION_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Initialized\n"
     ]
    }
   ],
   "source": [
    "embedding = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "vectorstore = AstraDBVectorStore(\n",
    "    collection_name = 'test',\n",
    "    embedding = embedding,\n",
    "    token = os.getenv('ASTRA_DB_APPLICATION_TOKEN'),\n",
    "    api_endpoint = os.getenv('ASTRA_DB_API_ENDPOINT')\n",
    ")\n",
    "\n",
    "print('Vector Store Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = os.path.join(os.getcwd(), 'Files')\n",
    "\n",
    "loader = PyPDFDirectoryLoader(FILE_PATH)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_ids = vectorstore.add_documents(final_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserted 79 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'documents': [{'_id': '159f19357fbe4ad2b555b04209cff5d4', 'content': 'allowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 8}}, {'_id': 'b104f87a68b948e899d54383417fdd90', 'content': 'Appendix\\nA. Contributions and Acknowledgments\\nCore Contributors\\nDaya Guo\\nDejian Yang\\nHaowei Zhang\\nJunxiao Song\\nRuoyu Zhang\\nRunxin Xu\\nQihao Zhu\\nShirong Ma\\nPeiyi Wang\\nXiao Bi\\nXiaokang Zhang\\nXingkai Yu\\nYu Wu\\nZ.F. Wu\\nZhibin Gou\\nZhihong Shao\\nZhuoshu Li\\nZiyi Gao\\nContributors\\nAixin Liu\\nBing Xue\\nBingxuan Wang\\nBochao Wu\\nBei Feng\\nChengda Lu\\nChenggang Zhao\\nChengqi Deng\\nChong Ruan\\nDamai Dai\\nDeli Chen\\nDongjie Ji\\nErhang Li\\nFangyun Lin\\nFucong Dai\\nFuli Luo*\\nGuangbo Hao\\nGuanting Chen\\nGuowei Li\\nH. Zhang\\nHanwei Xu\\nHonghui Ding\\nHuazuo Gao\\nHui QuHui Li\\nJianzhong Guo\\nJiashi Li\\nJingchang Chen\\nJingyang Yuan\\nJinhao Tu\\nJunjie Qiu\\nJunlong Li\\nJ.L. Cai\\nJiaqi Ni\\nJian Liang\\nJin Chen\\nKai Dong\\nKai Hu*\\nKaichao You\\nKaige Gao\\nKang Guan\\nKexin Huang\\nKuai Yu\\nLean Wang\\nLecong Zhang\\nLiang Zhao\\nLitong Wang\\nLiyue Zhang\\nLei Xu\\nLeyi Xia\\nMingchuan Zhang\\nMinghua Zhang\\nMinghui Tang\\nMingxu Zhou\\nMeng Li\\nMiaojun Wang\\nMingming Li\\nNing Tian\\nPanpan Huang\\nPeng Zhang\\nQiancheng Wang\\nQinyu Chen\\nQiushi Du\\nRuiqi Ge*\\nRuisong Zhang\\nRuizhe Pan', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 19}}, {'_id': '4b95fb2ebc2546f281bbd8892a5dcc93', 'content': '4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion, Limitations, and Future Work 16\\nA Contributions and Acknowledgments 20\\n2', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 1}}, {'_id': '929c7a9ed2d64082b6731bcfb4a8723a', 'content': 'model’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.\\nReasoning data We curate reasoning prompts and generate reasoning trajectories by perform-\\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\\nwe expand the dataset by incorporating additional data, some of which use a generative reward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\\nabout 600k reasoning related training samples.\\n10', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 9}}, {'_id': '147c2ff1c6f549cbb054b760ab5143ac', 'content': 'DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 12}}, {'_id': '7db5f932eb2941ebbdb80f964844063b', 'content': 'as the proportion of target language words in the CoT. Although ablation experiments show\\nthat such alignment results in a slight degradation in the model’s performance, this reward\\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\\nreasoning tasks and the reward for language consistency by directly summing them to form the\\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\\non reasoning tasks.\\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\\nmodel’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 9}}, {'_id': '674e4bc3635149358048df8ad7549e32', 'content': 'Contents\\n1 Introduction 3\\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Approach 5\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5\\n2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9\\n2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 1}}, {'_id': 'c137eb71397743e7bbb5f1c4d6ce8072', 'content': 'assessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\\nresponse of the model, including both the reasoning process and the summary, to identify and\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\\n2.4. Distillation: Empower Small Models with Reasoning Capability\\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 10}}, {'_id': '4c461f31ec3546c5a0df08eabbefce6a', 'content': 'behaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 7}}, {'_id': '9f7d5e681c804cc9b4e2e9dfaf3cce10', 'content': 'learning stage aimed at improving the model’s helpfulness and harmlessness while simultane-\\nously refining its reasoning capabilities. Specifically, we train the model using a combination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 10}}, {'_id': 'f9d0fa5b7cb24068a9ad04ae10bf121a', 'content': 'and the value model, iteratively refining the process.\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n15', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 14}}, {'_id': 'b4fbd575c1b147e8b618fb302c070468', 'content': 'reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 2}}, {'_id': '5bab8321c8784df4b0b23868d83837b2', 'content': 'massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974 .\\n17', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 16}}, {'_id': 'd73b077b083f4532b3bb957fd4b82783', 'content': 'Pushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD. Kumaran, T. Graepel, T. P . Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and\\nshogi by self-play with a general reinforcement learning algorithm. CoRR , abs/1712.01815,\\n2017a. URL http://arxiv.org/abs/1712.01815 .\\n18', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 17}}, {'_id': '265a333b690046ad8c0d86ec47fe5457', 'content': 'illustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 7}}, {'_id': 'a97ed5fe63464f4fbdcb2d878f9addf6', 'content': 'heavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data ,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1,𝑜2,···,𝑜𝐺}from the old\\npolicy𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺\\n𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\\n1\\n𝐺𝐺∑︁\\n𝑖=1\\x12', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 4}}, {'_id': 'acfb13069a43402981cd83521caf8462', 'content': 'tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 3}}, {'_id': '788e1f791b4e42fb8077b3eb5a2145cf', 'content': 'types of rewards:\\n•Accuracy rewards : The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n•Format rewards : In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training Template', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 5}}, {'_id': '7a18fe839e9d4688a7b1e3f6cdd34a3e', 'content': 'results. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?\\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\\n14', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 13}}, {'_id': '1dc3c078c6314498a0aae02202238e1d', 'content': 'improved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data ,', 'metadata': {'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 4}}], 'nextPageState': 'KQAAAAEBAAAAIDFkYzNjMDc4YzYzMTQ0OThhMGFhZTAyMjAyMjM4ZTFkAPB////rAA=='}}\n"
     ]
    }
   ],
   "source": [
    "# Checks your collection to verify the documents are embedded.\n",
    "print(vectorstore.astra_db.collection(\"test\").find())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs = {\n",
    "        'k' : 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 8}, page_content='models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.\\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9'),\n",
       " Document(metadata={'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 2}, page_content='mixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-'),\n",
       " Document(metadata={'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 0}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models'),\n",
       " Document(metadata={'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 6}, page_content='a clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7'),\n",
       " Document(metadata={'source': 'f:\\\\My-Learning-Related-Stuffs\\\\New_Learning\\\\Generative AI\\\\DataStax-AstraDB\\\\Files\\\\2501.12948v1.pdf', 'page': 8}, page_content='amount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('What is deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model='deepseek-r1:1.5b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context' : retriever, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke('how deepseek r1 is trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, I need to figure out what DeepSeek-R1 is trained with based on the provided context. Let me go through each document and section step by step.\\n\\nFirst document: This one talks about cold start data for a model. It mentions generating CoT data to fine-tune without the base model. So this shows that they're using specific types of answers from the target model (DeepSeek-R1) for training purposes.\\n\\nSecond document: This is about training templates. It says DeepSeek-R1-Zero uses a template that first produces reasoning processes followed by final answers. They mention avoiding content biases, so maybe their training data doesn't have any explicit instructions except what's necessary based on the task.\\n\\nThird document: This describes cold start data collection, focusing on generating detailed answers and using post-processing by humans to refine results. It also talks about reinforcement learning and retraining the reward model. So again, they're collecting data that's likely from DeepSeek-R1.\\n\\nFourth document: This is another part of cold start data, specifically mentioning the AIME benchmark. They collect thousands of cold-start data samples for fine-tuning.\\n\\nFifth document: This compares cold-start data to previous versions like R1-Zero. It mentions that in this case, they're collecting data from DeepSeek-R1-Zero's outputs rather than a different model. So this reinforces the idea that they're using their target model's responses as training data.\\n\\nSixth document: This talks about performance trajectories and response lengths during RL training. They mention extended computation for longer reasoning tasks, which shows how much time is required but doesn't directly answer the question of what they're trained with.\\n\\nSeventh document: This describes self-evolution behaviors like reflection and exploration through test-time computation. It emphasizes that this happens naturally as the model learns over time without explicit instructions.\\n\\nEighth document: This compares different models, showing that DeepSeek-R1-Zero uses large-scale training on various data types (math, code, STEM) and demonstrates superior performance in mathematics tasks. The focus is on what it's trained with in terms of data sources.\\n\\nSo putting it all together, the context shows that DeepSeek-R1 is trained using a variety of cold-start data, including responses from its target model (DeepSeek-R1), specific data types (math, code, STEM), and through reinforcement learning. The training process emphasizes self-evolution and uses large-scale datasets to improve performance across different tasks.\\n</think>\\n\\nDeepSeek-R1 is trained with several types of data, including responses from its target model (DeepSeek-R1) as cold-start data for fine-tuning. It also utilizes specific data types such as math, code, and STEM content, employing reinforcement learning and large-scale datasets to enhance performance across various tasks.\\n\\n**Answer:** DeepSeek-R1 is trained using responses from its target model, specific data types like math, code, and STEM, and through reinforcement learning with large-scale datasets.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

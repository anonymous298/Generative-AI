{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI possible?\n",
      "\n",
      "AI could allow us to create intelligent machines that act in the most efficient and effective ways, thereby saving lives.\n",
      "\n",
      "In the world of nanotechnology, it's not clear that this could happen anytime soon. We've been trying to create robots based on a variety of approaches, from DNA testing to machine learning. But, as the future of health care technology improves, so will the risks of using AI. There's no guarantee, though, that even the most benign of approaches to AI can do far enough to make this even less likely. But even if this happens, it would still have a serious negative impact on human health.\n",
      "\n",
      "A better way\n",
      "\n",
      "Even though we're living in a world of limited AI—which is not as clear-cut as life does—to be the closest thing we can come to fully human machines, there are numerous potential ethical considerations, not least the long term side effects. What is AI capable of? Is it capable of achieving \"perfect\" humans? And how much should we really consider the cost or pleasure associated with all that AI?\n",
      "\n",
      "This question was first posed by a colleague, Alan Sivak, at the start of this year's Google DeepMind conference — in 2013, during a series of Google DeepMind conferences. Sivak discussed ethics: \"We use different kinds of AI, and there are more ethical models out there, like natural selection and quantum computation [and] it could be much greater than just human.\" Even if it makes sense for us to spend millions on things like artificial blood transplants, we're living in a golden age of artificial intelligence.\n",
      "\n",
      "Sivak noted that there are many questions that still remain in the ethics of AI, from cost, to risk, to risk of harm — and some of these questions are quite vague and unclear:\n",
      "\n",
      "Is the goal? The goal is to enable us to work with different kinds of AI in a very efficient and safe way, but to do so, you have to make some promises in your product, to the extent that it would actually save lives. (It's clear that I'm going to use the words 'if' and 'impossible,' which makes sense.) Is the value of the technology used? Yes, there's cost. (In some ways, if you want to have it do better than you would with something that is not humanlike, you should ask the money maker to make a\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "chunks = generator('What is AI', max_length=500,)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
